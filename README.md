# ReAct_Simulation

This project provides a simple simulation of various prompting methods for large language models (LLMs) as described in the research paper "ReAct: Synergizing Reasoning and Acting in Language Models."

## Overview

This repository demonstrates how different prompting techniques can be used to enhance the reasoning and decision-making capabilities of LLMs. The methods simulated include:

1. **Standard Prompting**: Provides direct answers based on given context.
2. **Chain-of-Thought Prompting**: Breaks down problems into step-by-step reasoning.
3. **Act-Only Prompting**: Focuses on generating actions without explicit reasoning.
4. **ReAct Prompting**: Combines reasoning and acting in an interleaved manner.


### Example Output

The script will output the simulated responses for each method based on a predefined context and question.

## Methods Explained

### 1. Standard Prompting

Provides a direct answer based on the context without any intermediate reasoning or actions.

### 2. Chain-of-Thought Prompting

Simulates step-by-step reasoning by breaking down the problem into smaller logical steps.

### 3. Act-Only Prompting

Focuses on simulating actions based on the question without explicit reasoning steps.

### 4. ReAct Prompting

Combines both reasoning and acting by interleaving thoughts and actions, allowing dynamic updates based on observations.

## Acknowledgments

Inspired by the research paper "ReAct: Synergizing Reasoning and Acting in Language Models" by Shunyu Yao et al.
